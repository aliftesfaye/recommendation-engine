{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4316cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c95f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d262bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\recommendation-engine'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dff1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    model_name: str\n",
    "    embedding_dim: int\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    alpha: float\n",
    "    k: int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "634212d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hybrid_recommender.constants import *\n",
    "from src.hybrid_recommender.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998c2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.HybridRecommender\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        return ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path=config.train_data_path,\n",
    "            model_name=config.model_name,\n",
    "            embedding_dim=params.embedding_dim,\n",
    "            epochs=params.epochs,\n",
    "            learning_rate=params.learning_rate,\n",
    "            alpha=params.alpha,\n",
    "            k=params.k\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e5309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newst\\miniconda3\\envs\\recommendation-engine-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "from src.hybrid_recommender import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577a6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommender:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.user_encoder = None\n",
    "        self.item_encoder = None\n",
    "        self.organizer_encoder = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"Prepare data for training with event recommendation data\"\"\"\n",
    "        # Create encoders\n",
    "        self.user_encoder = {u: i for i, u in enumerate(df['user_id'].unique())}\n",
    "        self.item_encoder = {m: i for i, m in enumerate(df['event_id'].unique())}\n",
    "        self.organizer_encoder = {o: i for i, o in enumerate(df['organizer_id'].unique())}\n",
    "        \n",
    "        # Create inverse mappings\n",
    "        self.user_decoder = {i: u for u, i in self.user_encoder.items()}\n",
    "        self.item_decoder = {i: m for m, i in self.item_encoder.items()}\n",
    "        self.organizer_decoder = {i: o for o, i in self.organizer_encoder.items()}\n",
    "        \n",
    "        # Map IDs to encoded values\n",
    "        df['user_id_encoded'] = df['user_id'].map(self.user_encoder)\n",
    "        df['event_id_encoded'] = df['event_id'].map(self.item_encoder)\n",
    "        df['organizer_id_encoded'] = df['organizer_id'].map(self.organizer_encoder)\n",
    "        \n",
    "        # Create interaction score (weighted combination of different interactions)\n",
    "        df['interaction_score'] = (\n",
    "            df['is_booked'] * 0.5 + \n",
    "            df['num_bookings'] * 0.1 +\n",
    "            df['is_liked'] * 0.2 +\n",
    "            df['num_likes'] * 0.05 +\n",
    "            df['is_commented'] * 0.3 +\n",
    "            df['num_comments'] * 0.1\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def build_hybrid_model(self, n_users, n_items, n_organizers):\n",
    "        \"\"\"Build hybrid neural network model with organizer information\"\"\"\n",
    "        # User embedding path\n",
    "        user_input = Input(shape=(1,), name='user_input')\n",
    "        user_embedding = Embedding(n_users, self.config.embedding_dim, name='user_embedding')(user_input)\n",
    "        user_vec = Flatten(name='user_flatten')(user_embedding)\n",
    "        \n",
    "        # Event embedding path\n",
    "        event_input = Input(shape=(1,), name='event_input')\n",
    "        event_embedding = Embedding(n_items, self.config.embedding_dim, name='event_embedding')(event_input)\n",
    "        event_vec = Flatten(name='event_flatten')(event_embedding)\n",
    "        \n",
    "        # Organizer embedding path\n",
    "        organizer_input = Input(shape=(1,), name='organizer_input')\n",
    "        organizer_embedding = Embedding(n_organizers, self.config.embedding_dim//2, name='organizer_embedding')(organizer_input)\n",
    "        organizer_vec = Flatten(name='organizer_flatten')(organizer_embedding)\n",
    "        \n",
    "        # Dot product of user and event embeddings\n",
    "        dot_product = Dot(axes=1, name='dot_product')([user_vec, event_vec])\n",
    "        \n",
    "        # Combine with organizer information\n",
    "        merged = Concatenate()([dot_product, organizer_vec])\n",
    "        \n",
    "        # Add dense layers\n",
    "        dense = Dense(32, activation='relu')(merged)\n",
    "        dense = Dense(16, activation='relu')(dense)\n",
    "        \n",
    "        # Final output\n",
    "        output = Dense(1, activation='sigmoid', name='output')(dense)\n",
    "        \n",
    "        model = Model(inputs=[user_input, event_input, organizer_input], outputs=output)\n",
    "        model.compile(optimizer=Adam(learning_rate=self.config.learning_rate), \n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train_implicit_model(self, user_items):\n",
    "        \"\"\"Train ALS model from implicit\"\"\"\n",
    "        # Apply BM25 weighting\n",
    "        weighted = bm25_weight(user_items, K1=100, B=0.8)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = AlternatingLeastSquares(\n",
    "            factors=self.config.embedding_dim,\n",
    "            iterations=self.config.epochs,\n",
    "            regularization=self.config.alpha,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(weighted)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train hybrid recommendation system\"\"\"\n",
    "        # Load and prepare data\n",
    "        df = pd.read_csv(self.config.train_data_path)\n",
    "        df = self.prepare_data(df)\n",
    "        \n",
    "        # Create user-item matrix for implicit\n",
    "        user_items = csr_matrix(\n",
    "            (df['interaction_score'].values,\n",
    "             (df['user_id_encoded'], df['event_id_encoded'])),\n",
    "            shape=(len(self.user_encoder), len(self.item_encoder))\n",
    "        )\n",
    "        \n",
    "        # Train implicit ALS model\n",
    "        logger.info(\"Training implicit ALS model...\")\n",
    "        implicit_model = self.train_implicit_model(user_items)\n",
    "        \n",
    "        # Train neural network model\n",
    "        logger.info(\"Training neural network hybrid model...\")\n",
    "        n_users = len(self.user_encoder)\n",
    "        n_items = len(self.item_encoder)\n",
    "        n_organizers = len(self.organizer_encoder)\n",
    "        \n",
    "        nn_model = self.build_hybrid_model(n_users, n_items, n_organizers)\n",
    "        \n",
    "        # Prepare data for NN\n",
    "        X_user = df['user_id_encoded'].values\n",
    "        X_event = df['event_id_encoded'].values\n",
    "        X_organizer = df['organizer_id_encoded'].values\n",
    "        y = df['interaction_score'].values\n",
    "        \n",
    "        # Normalize interaction scores\n",
    "        y = self.scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Train NN\n",
    "        nn_model.fit(\n",
    "            [X_user, X_event, X_organizer], y,\n",
    "            epochs=self.config.epochs,\n",
    "            batch_size=64,\n",
    "            validation_split=0.1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save models\n",
    "        logger.info(\"Saving models...\")\n",
    "        models_dir = self.config.root_dir\n",
    "        joblib.dump(implicit_model, os.path.join(models_dir, 'implicit_model.joblib'))\n",
    "        nn_model.save(os.path.join(models_dir, 'nn_model.h5'))\n",
    "        joblib.dump(self.scaler, os.path.join(models_dir, 'scaler.joblib'))\n",
    "        joblib.dump({\n",
    "            'user_encoder': self.user_encoder,\n",
    "            'item_encoder': self.item_encoder,\n",
    "            'organizer_encoder': self.organizer_encoder\n",
    "        }, os.path.join(models_dir, 'encoders.joblib'))\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "\n",
    "    def recommend(self, user_id, n_recommendations=10):\n",
    "        \"\"\"Generate recommendations for a user\"\"\"\n",
    "        # Load models\n",
    "        models_dir = self.config.root_dir\n",
    "        implicit_model = joblib.load(os.path.join(models_dir, 'implicit_model.joblib'))\n",
    "        nn_model = tf.keras.models.load_model(os.path.join(models_dir, 'nn_model.h5'))\n",
    "        scaler = joblib.load(os.path.join(models_dir, 'scaler.joblib'))\n",
    "        encoders = joblib.load(os.path.join(models_dir, 'encoders.joblib'))\n",
    "        \n",
    "        self.user_encoder = encoders['user_encoder']\n",
    "        self.item_encoder = encoders['item_encoder']\n",
    "        self.organizer_encoder = encoders['organizer_encoder']\n",
    "        self.user_decoder = {i: u for u, i in self.user_encoder.items()}\n",
    "        self.item_decoder = {i: m for m, i in self.item_encoder.items()}\n",
    "        self.organizer_decoder = {i: o for o, i in self.organizer_encoder.items()}\n",
    "        \n",
    "        # Get user encoded ID\n",
    "        user_encoded = self.user_encoder.get(user_id)\n",
    "        if user_encoded is None:\n",
    "            return []  # Return empty list for cold-start users\n",
    "        \n",
    "        # Get implicit recommendations\n",
    "        implicit_recs = implicit_model.recommend(\n",
    "            user_encoded, \n",
    "            coo_matrix((len(self.user_encoder), len(self.item_encoder))),\n",
    "            N=n_recommendations*3  # Get more candidates for NN to score\n",
    "        )\n",
    "        \n",
    "        # Get organizer IDs for the recommended events\n",
    "        recommended_events = [self.item_decoder[item] for item in implicit_recs[0]]\n",
    "        # In a real implementation, you'd need a way to get organizer_id for these events\n",
    "        \n",
    "        # For demo purposes, we'll create dummy organizer IDs\n",
    "        organizer_array = np.random.randint(0, len(self.organizer_encoder), size=len(implicit_recs[0]))\n",
    "        \n",
    "        # Score with neural network\n",
    "        user_array = np.array([user_encoded] * len(implicit_recs[0]))\n",
    "        event_array = np.array(implicit_recs[0])\n",
    "        \n",
    "        nn_scores = nn_model.predict([user_array, event_array, organizer_array])\n",
    "        nn_scores = scaler.inverse_transform(nn_scores.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Combine and sort (weighted combination of implicit and NN scores)\n",
    "        combined_scores = implicit_recs[1] * 0.6 + nn_scores * 0.4\n",
    "        top_indices = np.argsort(combined_scores)[::-1][:n_recommendations]\n",
    "        \n",
    "        # Return recommendations with scores\n",
    "        recommendations = []\n",
    "        for idx in top_indices:\n",
    "            event_id = self.item_decoder[implicit_recs[0][idx]]\n",
    "            score = combined_scores[idx]\n",
    "            recommendations.append((event_id, score))\n",
    "        \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "871b1e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 23:18:58,497: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-06-30 23:18:58,500: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-06-30 23:18:58,502: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-06-30 23:18:58,503: INFO: common: created directory at: artifacts]\n",
      "[2025-06-30 23:18:58,503: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2025-06-30 23:18:58,787: INFO: 302607816: Training implicit ALS model...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newst\\miniconda3\\envs\\recommendation-engine-env\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: Intel MKL BLAS is configured to use 6 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'MKL_NUM_THREADS=1' or by callng 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having MKL use a threadpool can lead to severe performance issues\n",
      "  check_blas_config()\n",
      "d:\\newst\\miniconda3\\envs\\recommendation-engine-env\\Lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0 seconds\n",
      "  warnings.warn(\n",
      "100%|██████████| 20/20 [00:00<00:00, 51.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 23:18:59,320: INFO: 302607816: Training neural network hybrid model...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.2967 - loss: 0.3707 - val_accuracy: 0.3015 - val_loss: 0.3006\n",
      "Epoch 2/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.3032 - loss: 0.2784 - val_accuracy: 0.3015 - val_loss: 0.3148\n",
      "Epoch 3/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.3019 - loss: 0.2548 - val_accuracy: 0.3015 - val_loss: 0.3110\n",
      "Epoch 4/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3014 - loss: 0.2418 - val_accuracy: 0.3015 - val_loss: 0.3177\n",
      "Epoch 5/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3014 - loss: 0.2419 - val_accuracy: 0.3015 - val_loss: 0.3171\n",
      "Epoch 6/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.3007 - loss: 0.2418 - val_accuracy: 0.3015 - val_loss: 0.3165\n",
      "Epoch 7/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.3015 - loss: 0.2402 - val_accuracy: 0.3015 - val_loss: 0.3156\n",
      "Epoch 8/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3042 - loss: 0.2390 - val_accuracy: 0.3015 - val_loss: 0.3143\n",
      "Epoch 9/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.3006 - loss: 0.2402 - val_accuracy: 0.3015 - val_loss: 0.3130\n",
      "Epoch 10/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3028 - loss: 0.2382 - val_accuracy: 0.3015 - val_loss: 0.3142\n",
      "Epoch 11/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2998 - loss: 0.2392 - val_accuracy: 0.3015 - val_loss: 0.3138\n",
      "Epoch 12/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3042 - loss: 0.2375 - val_accuracy: 0.3015 - val_loss: 0.3142\n",
      "Epoch 13/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.3048 - loss: 0.2378 - val_accuracy: 0.3015 - val_loss: 0.3130\n",
      "Epoch 14/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3030 - loss: 0.2388 - val_accuracy: 0.3015 - val_loss: 0.3114\n",
      "Epoch 15/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3005 - loss: 0.2392 - val_accuracy: 0.3015 - val_loss: 0.3124\n",
      "Epoch 16/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3041 - loss: 0.2374 - val_accuracy: 0.3015 - val_loss: 0.3114\n",
      "Epoch 17/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.3027 - loss: 0.2381 - val_accuracy: 0.3015 - val_loss: 0.3113\n",
      "Epoch 18/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3018 - loss: 0.2390 - val_accuracy: 0.3015 - val_loss: 0.3098\n",
      "Epoch 19/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3007 - loss: 0.2392 - val_accuracy: 0.3015 - val_loss: 0.3100\n",
      "Epoch 20/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3005 - loss: 0.2384 - val_accuracy: 0.3015 - val_loss: 0.3096\n",
      "[2025-06-30 23:24:33,390: INFO: 302607816: Saving models...]\n",
      "[2025-06-30 23:24:33,396: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n",
      "[2025-06-30 23:24:33,508: INFO: 302607816: Training completed!]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    recommender_config = config.get_model_trainer_config()\n",
    "    recommender = HybridRecommender(config=recommender_config)\n",
    "    recommender.train()\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error in training hybrid recommender\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fbfc76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 23:24:33,591: WARNING: saving_utils: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.]\n"
     ]
    }
   ],
   "source": [
    "recommendations = recommender.recommend(user_id=12345, n_recommendations=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation-engine-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
