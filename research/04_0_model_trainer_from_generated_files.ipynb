{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4316cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c95f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d262bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\recommendation-engine'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dff1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    model_name: str\n",
    "    embedding_dim: int\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    alpha: float\n",
    "    k: int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "634212d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hybrid_recommender.constants import *\n",
    "from src.hybrid_recommender.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998c2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.HybridRecommender\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        return ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path=config.train_data_path,\n",
    "            model_name=config.model_name,\n",
    "            embedding_dim=params.embedding_dim,\n",
    "            epochs=params.epochs,\n",
    "            learning_rate=params.learning_rate,\n",
    "            alpha=params.alpha,\n",
    "            k=params.k\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e5309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newst\\miniconda3\\envs\\recommendation-engine-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "from src.hybrid_recommender import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommender:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.user_encoder = None\n",
    "        self.item_encoder = None\n",
    "        self.organizer_encoder = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"Prepare data for training with event recommendation data\"\"\"\n",
    "        self.user_encoder = {u: i for i, u in enumerate(df['user_id'].unique())}\n",
    "        self.item_encoder = {m: i for i, m in enumerate(df['event_id'].unique())}\n",
    "        self.organizer_encoder = {o: i for i, o in enumerate(df['organizer_id'].unique())}\n",
    "        \n",
    "        self.user_decoder = {i: u for u, i in self.user_encoder.items()}\n",
    "        self.item_decoder = {i: m for m, i in self.item_encoder.items()}\n",
    "        self.organizer_decoder = {i: o for o, i in self.organizer_encoder.items()}\n",
    "        \n",
    "        df['user_id_encoded'] = df['user_id'].map(self.user_encoder)\n",
    "        df['event_id_encoded'] = df['event_id'].map(self.item_encoder)\n",
    "        df['organizer_id_encoded'] = df['organizer_id'].map(self.organizer_encoder)\n",
    "        \n",
    "        df['interaction_score'] = (\n",
    "            df['is_booked'] * 0.5 + \n",
    "            df['num_bookings'] * 0.1 +\n",
    "            df['is_liked'] * 0.2 +\n",
    "            df['num_likes'] * 0.05 +\n",
    "            df['is_commented'] * 0.3 +\n",
    "            df['num_comments'] * 0.1\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def build_hybrid_model(self, n_users, n_items, n_organizers):\n",
    "        \"\"\"Build hybrid neural network model with organizer information\"\"\"\n",
    "        user_input = Input(shape=(1,), name='user_input')\n",
    "        user_embedding = Embedding(n_users, self.config.embedding_dim, name='user_embedding')(user_input)\n",
    "        user_vec = Flatten(name='user_flatten')(user_embedding)\n",
    "        \n",
    "        event_input = Input(shape=(1,), name='event_input')\n",
    "        event_embedding = Embedding(n_items, self.config.embedding_dim, name='event_embedding')(event_input)\n",
    "        event_vec = Flatten(name='event_flatten')(event_embedding)\n",
    "        \n",
    "        organizer_input = Input(shape=(1,), name='organizer_input')\n",
    "        organizer_embedding = Embedding(n_organizers, self.config.embedding_dim//2, name='organizer_embedding')(organizer_input)\n",
    "        organizer_vec = Flatten(name='organizer_flatten')(organizer_embedding)\n",
    "        \n",
    "        dot_product = Dot(axes=1, name='dot_product')([user_vec, event_vec])\n",
    "        \n",
    "        merged = Concatenate()([dot_product, organizer_vec])\n",
    "        \n",
    "        dense = Dense(32, activation='relu')(merged)\n",
    "        dense = Dense(16, activation='relu')(dense)\n",
    "        \n",
    "        output = Dense(1, activation='sigmoid', name='output')(dense)\n",
    "        \n",
    "        model = Model(inputs=[user_input, event_input, organizer_input], outputs=output)\n",
    "        model.compile(optimizer=Adam(learning_rate=self.config.learning_rate), \n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train_implicit_model(self, user_items):\n",
    "        \"\"\"Train ALS model from implicit\"\"\"\n",
    "        \n",
    "        weighted = bm25_weight(user_items, K1=100, B=0.8)\n",
    "        \n",
    "        model = AlternatingLeastSquares(\n",
    "            factors=self.config.embedding_dim,\n",
    "            iterations=self.config.epochs,\n",
    "            regularization=self.config.alpha,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        model.fit(weighted)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        \"\"\"Train hybrid recommendation system\"\"\"\n",
    "        \n",
    "        df = pd.read_csv(self.config.train_data_path)\n",
    "        df = self.prepare_data(df)\n",
    "        \n",
    "        user_items = csr_matrix(\n",
    "            (df['interaction_score'].values,\n",
    "             (df['user_id_encoded'], df['event_id_encoded'])),\n",
    "            shape=(len(self.user_encoder), len(self.item_encoder))\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Training implicit ALS model...\")\n",
    "        implicit_model = self.train_implicit_model(user_items)\n",
    "        \n",
    "        logger.info(\"Training neural network hybrid model...\")\n",
    "        n_users = len(self.user_encoder)\n",
    "        n_items = len(self.item_encoder)\n",
    "        n_organizers = len(self.organizer_encoder)\n",
    "        \n",
    "        nn_model = self.build_hybrid_model(n_users, n_items, n_organizers)\n",
    "        \n",
    "        X_user = df['user_id_encoded'].values\n",
    "        X_event = df['event_id_encoded'].values\n",
    "        X_organizer = df['organizer_id_encoded'].values\n",
    "        y = df['interaction_score'].values\n",
    "        \n",
    "        y = self.scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        nn_model.fit(\n",
    "            [X_user, X_event, X_organizer], y,\n",
    "            epochs=self.config.epochs,\n",
    "            batch_size=64,\n",
    "            validation_split=0.1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save models\n",
    "        logger.info(\"Saving models...\")\n",
    "        models_dir = self.config.root_dir\n",
    "        joblib.dump(implicit_model, os.path.join(models_dir, 'implicit_model.joblib'))\n",
    "        nn_model.save(os.path.join(models_dir, 'nn_model.h5'))\n",
    "        joblib.dump(self.scaler, os.path.join(models_dir, 'scaler.joblib'))\n",
    "        joblib.dump({\n",
    "            'user_encoder': self.user_encoder,\n",
    "            'item_encoder': self.item_encoder,\n",
    "            'organizer_encoder': self.organizer_encoder\n",
    "        }, os.path.join(models_dir, 'encoders.joblib'))\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "871b1e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 15:09:57,586: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-07-01 15:09:57,586: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-07-01 15:09:57,586: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-07-01 15:09:57,586: INFO: common: created directory at: artifacts]\n",
      "[2025-07-01 15:09:57,586: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2025-07-01 15:09:57,903: INFO: 2886064907: Training implicit ALS model...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newst\\miniconda3\\envs\\recommendation-engine-env\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: Intel MKL BLAS is configured to use 6 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'MKL_NUM_THREADS=1' or by callng 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having MKL use a threadpool can lead to severe performance issues\n",
      "  check_blas_config()\n",
      "d:\\newst\\miniconda3\\envs\\recommendation-engine-env\\Lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0 seconds\n",
      "  warnings.warn(\n",
      "100%|██████████| 20/20 [00:00<00:00, 54.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 15:09:58,404: INFO: 2886064907: Training neural network hybrid model...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.3047 - loss: 0.3397 - val_accuracy: 0.2990 - val_loss: 0.3015\n",
      "Epoch 2/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3019 - loss: 0.3001 - val_accuracy: 0.2990 - val_loss: 0.3021\n",
      "Epoch 3/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.3033 - loss: 0.2777 - val_accuracy: 0.2990 - val_loss: 0.3090\n",
      "Epoch 4/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.3014 - loss: 0.2582 - val_accuracy: 0.2990 - val_loss: 0.3153\n",
      "Epoch 5/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.3024 - loss: 0.2502 - val_accuracy: 0.2990 - val_loss: 0.3201\n",
      "Epoch 6/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.3010 - loss: 0.2480 - val_accuracy: 0.2990 - val_loss: 0.3271\n",
      "Epoch 7/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 10ms/step - accuracy: 0.3043 - loss: 0.2446 - val_accuracy: 0.2990 - val_loss: 0.3283\n",
      "Epoch 8/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.3013 - loss: 0.2437 - val_accuracy: 0.2990 - val_loss: 0.3365\n",
      "Epoch 9/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.3031 - loss: 0.2422 - val_accuracy: 0.2990 - val_loss: 0.3367\n",
      "Epoch 10/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3033 - loss: 0.2414 - val_accuracy: 0.2990 - val_loss: 0.3382\n",
      "Epoch 11/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.3031 - loss: 0.2409 - val_accuracy: 0.2990 - val_loss: 0.3387\n",
      "Epoch 12/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.3009 - loss: 0.2416 - val_accuracy: 0.2990 - val_loss: 0.3347\n",
      "Epoch 13/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.3007 - loss: 0.2413 - val_accuracy: 0.2990 - val_loss: 0.3370\n",
      "Epoch 14/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 11ms/step - accuracy: 0.3020 - loss: 0.2406 - val_accuracy: 0.2990 - val_loss: 0.3390\n",
      "Epoch 15/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 11ms/step - accuracy: 0.3038 - loss: 0.2399 - val_accuracy: 0.2990 - val_loss: 0.3366\n",
      "Epoch 16/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 10ms/step - accuracy: 0.3033 - loss: 0.2396 - val_accuracy: 0.2990 - val_loss: 0.3364\n",
      "Epoch 17/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.3021 - loss: 0.2401 - val_accuracy: 0.2990 - val_loss: 0.3383\n",
      "Epoch 18/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.3025 - loss: 0.2387 - val_accuracy: 0.2990 - val_loss: 0.3339\n",
      "Epoch 19/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3014 - loss: 0.2394 - val_accuracy: 0.2990 - val_loss: 0.3352\n",
      "Epoch 20/20\n",
      "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3033 - loss: 0.2391 - val_accuracy: 0.2990 - val_loss: 0.3325\n",
      "[2025-07-01 15:16:23,752: INFO: 2886064907: Saving models...]\n",
      "[2025-07-01 15:16:23,758: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n",
      "[2025-07-01 15:16:23,882: INFO: 2886064907: Training completed!]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    recommender_config = config.get_model_trainer_config()\n",
    "    recommender = HybridRecommender(config=recommender_config)\n",
    "    recommender.train()\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error in training hybrid recommender\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation-engine-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
