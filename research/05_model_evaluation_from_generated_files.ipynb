{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80e0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4c2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c575a996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\recommendation-engine'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdc4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    implicit_model_path: Path\n",
    "    nn_model_path: Path\n",
    "    scaler_path: Path\n",
    "    encoders_path: Path\n",
    "    all_params: dict\n",
    "    metrics_file_name: Path\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f169a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hybrid_recommender.constants import *\n",
    "from src.hybrid_recommender.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287d4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.HybridRecommender\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        return ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            implicit_model_path=config.implicit_model_path,\n",
    "            nn_model_path=config.nn_model_path,\n",
    "            scaler_path=config.scaler_path,\n",
    "            encoders_path=config.encoders_path,\n",
    "            all_params=params,\n",
    "            metrics_file_name=config.metrics_file_name,\n",
    "            k=params.k\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from typing import Dict, Tuple, List\n",
    "import json\n",
    "from scipy.sparse import csr_matrix\n",
    "from src.hybrid_recommender import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommenderEvaluator:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.load_models()\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load all required models and encoders\"\"\"\n",
    "        self.implicit_model = joblib.load(self.config.implicit_model_path)\n",
    "        self.nn_model = tf.keras.models.load_model(self.config.nn_model_path)\n",
    "        self.scaler = joblib.load(self.config.scaler_path)\n",
    "        encoders = joblib.load(self.config.encoders_path)\n",
    "        \n",
    "        self.user_encoder = encoders['user_encoder']\n",
    "        self.item_encoder = encoders['item_encoder']\n",
    "        self.organizer_encoder = encoders['organizer_encoder']\n",
    "        \n",
    "        # Create inverse mappings\n",
    "        self.user_decoder = {i: u for u, i in self.user_encoder.items()}\n",
    "        self.item_decoder = {i: m for m, i in self.item_encoder.items()}\n",
    "        self.organizer_decoder = {i: o for o, i in self.organizer_encoder.items()}\n",
    "\n",
    "    def evaluate_recommendations(self, test_data: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate recommendation quality using multiple metrics\"\"\"\n",
    "        # Filter test data to only include known users/events\n",
    "        test_data = test_data[\n",
    "            test_data['user_id'].isin(self.user_encoder) & \n",
    "            test_data['event_id'].isin(self.item_encoder)\n",
    "        ]\n",
    "        \n",
    "        # Group by user and get actual interactions\n",
    "        user_events = test_data.groupby('user_id')['event_id'].apply(set).to_dict()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'precision@k': [],\n",
    "            'recall@k': [],\n",
    "            'ndcg@k': [],\n",
    "            'map@k': [],\n",
    "            'coverage': self.calculate_coverage(test_data),\n",
    "            'popularity_bias': self.calculate_popularity_bias(test_data)\n",
    "        }\n",
    "        \n",
    "        for user_id, actual_events in user_events.items():\n",
    "            recommended = self._recommend(user_id)\n",
    "            metrics['precision@k'].append(self._precision(actual_events, recommended))\n",
    "            metrics['recall@k'].append(self._recall(actual_events, recommended))\n",
    "            metrics['ndcg@k'].append(self._ndcg(actual_events, recommended))\n",
    "            metrics['map@k'].append(self._average_precision(actual_events, recommended))\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        results = {\n",
    "            'precision@k': np.mean(metrics['precision@k']),\n",
    "            'recall@k': np.mean(metrics['recall@k']),\n",
    "            'ndcg@k': np.mean(metrics['ndcg@k']),\n",
    "            'map@k': np.mean(metrics['map@k']),\n",
    "            'coverage': metrics['coverage'],\n",
    "            'popularity_bias': metrics['popularity_bias'],\n",
    "            'f1_score': 2 * (metrics['precision@k'] * metrics['recall@k']) / \n",
    "                        (metrics['precision@k'] + metrics['recall@k'])\n",
    "                        if (metrics['precision@k'] + metrics['recall@k']) > 0 else 0\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _precision(self, actual: set, recommended: list) -> float:\n",
    "        \"\"\"Calculate precision@k\"\"\"\n",
    "        relevant = len(set(recommended) & actual)\n",
    "        return relevant / len(recommended) if recommended else 0\n",
    "\n",
    "    def _recall(self, actual: set, recommended: list) -> float:\n",
    "        \"\"\"Calculate recall@k\"\"\"\n",
    "        relevant = len(set(recommended) & actual)\n",
    "        return relevant / len(actual) if actual else 0\n",
    "\n",
    "    def _ndcg(self, actual: set, recommended: list) -> float:\n",
    "        \"\"\"Calculate Normalized Discounted Cumulative Gain\"\"\"\n",
    "        relevances = [1 if event in actual else 0 for event in recommended]\n",
    "        discounts = np.log2(np.arange(2, len(relevances) + 2))\n",
    "        dcg = np.sum(relevances / discounts)\n",
    "        ideal_relevances = [1] * min(len(actual), len(recommended))\n",
    "        idcg = np.sum(ideal_relevances / np.log2(np.arange(2, len(ideal_relevances) + 2)))\n",
    "        return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    def _average_precision(self, actual: set, recommended: list) -> float:\n",
    "        \"\"\"Calculate Average Precision\"\"\"\n",
    "        relevant = []\n",
    "        for i, event in enumerate(recommended):\n",
    "            if event in actual:\n",
    "                relevant.append(self._precision(actual, recommended[:i+1]))\n",
    "        return np.mean(relevant) if relevant else 0\n",
    "\n",
    "    def calculate_coverage(self, test_data: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate what percentage of events can be recommended\"\"\"\n",
    "        all_events = set(self.item_decoder.values())\n",
    "        recommended_events = set()\n",
    "        \n",
    "        for user_id in test_data['user_id'].unique():\n",
    "            recommended_events.update(self._recommend(user_id))\n",
    "        \n",
    "        return len(recommended_events) / len(all_events)\n",
    "\n",
    "    def calculate_popularity_bias(self, test_data: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate how biased recommendations are toward popular events\"\"\"\n",
    "        event_popularity = test_data['event_id'].value_counts().to_dict()\n",
    "        recommendations_popularity = []\n",
    "        \n",
    "        for user_id in test_data['user_id'].unique():\n",
    "            for event_id in self._recommend(user_id):\n",
    "                recommendations_popularity.append(event_popularity.get(event_id, 0))\n",
    "        \n",
    "        avg_rec_pop = np.mean(recommendations_popularity) if recommendations_popularity else 0\n",
    "        avg_all_pop = np.mean(list(event_popularity.values()))\n",
    "        \n",
    "        return avg_rec_pop / avg_all_pop if avg_all_pop > 0 else 0\n",
    "\n",
    "    def _recommend(self, user_id: int) -> List[str]:\n",
    "        \"\"\"Generate recommendations for a single user\"\"\"\n",
    "        user_encoded = self.user_encoder[user_id]\n",
    "        \n",
    "        user_items = csr_matrix((1, len(self.item_encoder)), dtype=np.float32)\n",
    "        \n",
    "        # Get implicit recommendations\n",
    "        implicit_recs = self.implicit_model.recommend(\n",
    "            userid=user_encoded,\n",
    "            user_items=user_items,\n",
    "            N=self.config.k * 3,\n",
    "            filter_already_liked_items=False\n",
    "        )\n",
    "        \n",
    "        # Get organizer IDs for recommended events\n",
    "        recommended_events = [self.item_decoder[item] for item in implicit_recs[0]]\n",
    "        organizer_ids = [self._get_organizer_for_event(event) for event in recommended_events]\n",
    "        organizer_encoded = [self.organizer_encoder.get(o, 0) for o in organizer_ids]\n",
    "        \n",
    "        # Score with neural network\n",
    "        user_array = np.array([user_encoded] * len(implicit_recs[0]))\n",
    "        event_array = np.array(implicit_recs[0])\n",
    "        organizer_array = np.array(organizer_encoded)\n",
    "        \n",
    "        nn_scores = self.nn_model.predict(\n",
    "            [user_array, event_array, organizer_array], \n",
    "            verbose=0\n",
    "        )\n",
    "        nn_scores = self.scaler.inverse_transform(nn_scores.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Combine and sort\n",
    "        combined_scores = implicit_recs[1] * 0.6 + nn_scores * 0.4\n",
    "        top_indices = np.argsort(combined_scores)[::-1][:self.config.k]\n",
    "        \n",
    "        return [self.item_decoder[implicit_recs[0][i]] for i in top_indices]\n",
    "\n",
    "    def _get_organizer_for_event(self, event_id: str) -> str:\n",
    "        \"\"\"Helper method to get organizer for an event\"\"\"\n",
    "        # Implement actual organizer lookup here\n",
    "        return str(hash(event_id) % 1000)\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Run evaluation and save metrics\"\"\"\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "        metrics = self.evaluate_recommendations(test_data)\n",
    "        \n",
    "        full_results = {\n",
    "            **metrics,\n",
    "            \"model_parameters\": self.config.all_params,\n",
    "            \"num_users\": len(self.user_encoder),\n",
    "            \"num_events\": len(self.item_encoder),\n",
    "            \"num_organizers\": len(self.organizer_encoder),\n",
    "            \"evaluation_time\": pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(self.config.metrics_file_name, 'w') as f:\n",
    "            json.dump(full_results, f, indent=4)\n",
    "        \n",
    "        logger.info(f\"Evaluation results saved to {self.config.metrics_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ffcae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 23:12:39,373: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-06-30 23:12:39,375: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-06-30 23:12:39,376: INFO: common: created directory at: artifacts]\n",
      "[2025-06-30 23:12:39,377: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2025-06-30 23:12:39,520: WARNING: saving_utils: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newst\\miniconda3\\envs\\recommendation-engine-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 23:12:39,778: ERROR: 1754327610: Error during model evaluation]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\newst\\AppData\\Local\\Temp\\ipykernel_4072\\1754327610.py\", line 5, in <module>\n",
      "    evaluator.save_results()\n",
      "  File \"C:\\Users\\newst\\AppData\\Local\\Temp\\ipykernel_4072\\548256017.py\", line 157, in save_results\n",
      "    metrics = self.evaluate_recommendations(test_data)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newst\\AppData\\Local\\Temp\\ipykernel_4072\\548256017.py\", line 39, in evaluate_recommendations\n",
      "    'coverage': self.calculate_coverage(test_data),\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newst\\AppData\\Local\\Temp\\ipykernel_4072\\548256017.py\", line 98, in calculate_coverage\n",
      "    recommended_events.update(self._recommend(user_id))\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newst\\AppData\\Local\\Temp\\ipykernel_4072\\548256017.py\", line 121, in _recommend\n",
      "    implicit_recs = self.implicit_model.recommend(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\newst\\miniconda3\\envs\\recommendation-engine-env\\Lib\\site-packages\\implicit\\cpu\\matrix_factorization_base.py\", line 46, in recommend\n",
      "    raise ValueError(\"user_items needs to be a CSR sparse matrix\")\n",
      "ValueError: user_items needs to be a CSR sparse matrix\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "user_items needs to be a CSR sparse matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      7\u001b[39m     logger.exception(\u001b[33m\"\u001b[39m\u001b[33mError during model evaluation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m     model_evaluation_config = config.get_model_evaluation_config()\n\u001b[32m      4\u001b[39m     evaluator = HybridRecommenderEvaluator(config=model_evaluation_config)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      7\u001b[39m     logger.exception(\u001b[33m\"\u001b[39m\u001b[33mError during model evaluation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mHybridRecommenderEvaluator.save_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run evaluation and save metrics\"\"\"\u001b[39;00m\n\u001b[32m    156\u001b[39m test_data = pd.read_csv(\u001b[38;5;28mself\u001b[39m.config.test_data_path)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m full_results = {\n\u001b[32m    160\u001b[39m     **metrics,\n\u001b[32m    161\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_parameters\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.config.all_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevaluation_time\u001b[39m\u001b[33m\"\u001b[39m: pd.Timestamp.now().isoformat()\n\u001b[32m    166\u001b[39m }\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m.config.metrics_file_name, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mHybridRecommenderEvaluator.evaluate_recommendations\u001b[39m\u001b[34m(self, test_data)\u001b[39m\n\u001b[32m     31\u001b[39m user_events = test_data.groupby(\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mevent_id\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28mset\u001b[39m).to_dict()\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[32m     34\u001b[39m metrics = {\n\u001b[32m     35\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprecision@k\u001b[39m\u001b[33m'\u001b[39m: [],\n\u001b[32m     36\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrecall@k\u001b[39m\u001b[33m'\u001b[39m: [],\n\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mndcg@k\u001b[39m\u001b[33m'\u001b[39m: [],\n\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmap@k\u001b[39m\u001b[33m'\u001b[39m: [],\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcoverage\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcalculate_coverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     40\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpopularity_bias\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.calculate_popularity_bias(test_data)\n\u001b[32m     41\u001b[39m }\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m user_id, actual_events \u001b[38;5;129;01min\u001b[39;00m user_events.items():\n\u001b[32m     44\u001b[39m     recommended = \u001b[38;5;28mself\u001b[39m._recommend(user_id)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mHybridRecommenderEvaluator.calculate_coverage\u001b[39m\u001b[34m(self, test_data)\u001b[39m\n\u001b[32m     95\u001b[39m recommended_events = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m user_id \u001b[38;5;129;01min\u001b[39;00m test_data[\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m].unique():\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     recommended_events.update(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recommend\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(recommended_events) / \u001b[38;5;28mlen\u001b[39m(all_events)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mHybridRecommenderEvaluator._recommend\u001b[39m\u001b[34m(self, user_id)\u001b[39m\n\u001b[32m    118\u001b[39m user_encoded = \u001b[38;5;28mself\u001b[39m.user_encoder[user_id]\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Get implicit recommendations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m implicit_recs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimplicit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecommend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mN\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m    125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Get organizer IDs for recommended events\u001b[39;00m\n\u001b[32m    128\u001b[39m recommended_events = [\u001b[38;5;28mself\u001b[39m.item_decoder[item] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m implicit_recs[\u001b[32m0\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\newst\\miniconda3\\envs\\recommendation-engine-env\\Lib\\site-packages\\implicit\\cpu\\matrix_factorization_base.py:46\u001b[39m, in \u001b[36mMatrixFactorizationBase.recommend\u001b[39m\u001b[34m(self, userid, user_items, N, filter_already_liked_items, filter_items, recalculate_user, items)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filter_already_liked_items \u001b[38;5;129;01mor\u001b[39;00m recalculate_user:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(user_items, csr_matrix):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33muser_items needs to be a CSR sparse matrix\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m     user_count = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.isscalar(userid) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(userid)\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_items.shape[\u001b[32m0\u001b[39m] != user_count:\n",
      "\u001b[31mValueError\u001b[39m: user_items needs to be a CSR sparse matrix"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    evaluator = HybridRecommenderEvaluator(config=model_evaluation_config)\n",
    "    evaluator.save_results()\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error during model evaluation\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation-engine-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
