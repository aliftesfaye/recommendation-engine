{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80e0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4c2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c575a996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\recommendation-engine'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdc4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    implicit_model_path: Path\n",
    "    nn_model_path: Path\n",
    "    scaler_path: Path\n",
    "    encoders_path: Path\n",
    "    all_params: dict\n",
    "    metrics_file_name: Path\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f169a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hybrid_recommender.constants import *\n",
    "from src.hybrid_recommender.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287d4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.HybridRecommender\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        return ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            implicit_model_path=config.implicit_model_path,\n",
    "            nn_model_path=config.nn_model_path,\n",
    "            scaler_path=config.scaler_path,\n",
    "            encoders_path=config.encoders_path,\n",
    "            all_params=params,\n",
    "            metrics_file_name=config.metrics_file_name,\n",
    "            k=params.k\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045f4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from typing import Dict, Tuple, List\n",
    "import json\n",
    "from scipy.sparse import csr_matrix\n",
    "from src.hybrid_recommender import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommenderEvaluator:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        logger.info(\"Initializing HybridRecommenderEvaluator\")\n",
    "        self.config = config\n",
    "        self.load_models()\n",
    "        logger.info(\"HybridRecommenderEvaluator initialized successfully\")\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load all required models and encoders\"\"\"\n",
    "        logger.info(\"Loading models and encoders\")\n",
    "        \n",
    "        try:\n",
    "            self.implicit_model = joblib.load(self.config.implicit_model_path)\n",
    "            logger.info(f\"Loaded implicit model from {self.config.implicit_model_path}\")\n",
    "            self.nn_model = tf.keras.models.load_model(self.config.nn_model_path)\n",
    "            logger.info(f\"Loaded neural network model from {self.config.nn_model_path}\")\n",
    "            self.scaler = joblib.load(self.config.scaler_path)\n",
    "            logger.info(f\"Loaded scaler from {self.config.scaler_path}\")\n",
    "            encoders = joblib.load(self.config.encoders_path)\n",
    "            logger.info(f\"Loaded encoders from {self.config.encoders_path}\")\n",
    "            \n",
    "            self.user_encoder = encoders['user_encoder']\n",
    "            self.item_encoder = encoders['item_encoder']\n",
    "            self.organizer_encoder = encoders['organizer_encoder']\n",
    "            logger.info(\"Successfully extracted user, item and organizer encoders\")\n",
    "            \n",
    "            self.user_decoder = {i: u for u, i in self.user_encoder.items()}\n",
    "            self.item_decoder = {i: m for m, i in self.item_encoder.items()}\n",
    "            self.organizer_decoder = {i: o for o, i in self.organizer_encoder.items()}\n",
    "            logger.info(\"Created inverse mappings for encoders\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading models: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "\n",
    "    def evaluate_recommendations(self, test_data: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate recommendation quality using multiple metrics\"\"\"\n",
    "        logger.info(\"Starting recommendation evaluation\")\n",
    "        \n",
    "        original_size = len(test_data)\n",
    "        test_data = test_data[\n",
    "            test_data['user_id'].isin(self.user_encoder) & \n",
    "            test_data['event_id'].isin(self.item_encoder)\n",
    "        ]\n",
    "        filtered_size = len(test_data)\n",
    "        logger.info(f\"Filtered test data from {original_size} to {filtered_size} records\")\n",
    "\n",
    "        \n",
    "        user_events = test_data.groupby('user_id')['event_id'].apply(set).to_dict()\n",
    "        logger.info(f\"Prepared evaluation data for {len(user_events)} users\")\n",
    "        \n",
    "        metrics = {\n",
    "            'precision@k': [],\n",
    "            'recall@k': [],\n",
    "            'ndcg@k': [],\n",
    "            'map@k': [],\n",
    "            'coverage': self.calculate_coverage(test_data),\n",
    "            'popularity_bias': self.calculate_popularity_bias(test_data)\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Initialized metrics dictionary\")\n",
    "        \n",
    "        for i, (user_id, actual_events) in enumerate(user_events.items()):\n",
    "            if i % 100 == 0:\n",
    "                logger.debug(f\"Processing user {i+1}/{len(user_events)}\")\n",
    "            recommended = self._recommend(user_id)\n",
    "            metrics['precision@k'].append(self._precision(actual_events, recommended))\n",
    "            metrics['recall@k'].append(self._recall(actual_events, recommended))\n",
    "            metrics['ndcg@k'].append(self._ndcg(actual_events, recommended))\n",
    "            metrics['map@k'].append(self._average_precision(actual_events, recommended))\n",
    "        \n",
    "        avg_precision = np.mean(metrics['precision@k'])\n",
    "        avg_recall = np.mean(metrics['recall@k'])\n",
    "        \n",
    "        results = {\n",
    "            'precision@k': avg_precision,\n",
    "            'recall@k': avg_recall,\n",
    "            'ndcg@k': np.mean(metrics['ndcg@k']),\n",
    "            'map@k': np.mean(metrics['map@k']),\n",
    "            'coverage': metrics['coverage'],\n",
    "            'popularity_bias': metrics['popularity_bias'],\n",
    "            'f1_score': 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) \n",
    "                if (avg_precision + avg_recall) > 0 else 0\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Evaluation completed. Metrics calculated:\")\n",
    "        for metric, value in results.items():\n",
    "            logger.info(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _precision(self, actual: set, recommended: list) -> float:\n",
    "        \"\"\"Calculate precision@k\"\"\"\n",
    "        relevant = len(set(recommended) & actual)\n",
    "        precision = relevant / len(recommended) if recommended else 0\n",
    "        logger.debug(f\"Precision calculation - relevant: {relevant}, recommended: {len(recommended)}, precision: {precision:.4f}\")  \n",
    "        return precision\n",
    "\n",
    "    def _recall(self, actual: set, recommended: list) -> float:\n",
    "        \"\"\"Calculate recall@k\"\"\"\n",
    "        relevant = len(set(recommended) & actual)\n",
    "        recall = relevant / len(actual) if actual else 0\n",
    "        logger.debug(f\"Recall calculation - relevant: {relevant}, actual: {len(actual)}, recall: {recall:.4f}\")\n",
    "        return recall\n",
    "\n",
    "    def _ndcg(self, actual: set, recommended: list) -> float:\n",
    "        \"\"\"Calculate Normalized Discounted Cumulative Gain\"\"\"\n",
    "        relevances = [1 if event in actual else 0 for event in recommended]\n",
    "        discounts = np.log2(np.arange(2, len(relevances) + 2))\n",
    "        dcg = np.sum(relevances / discounts)\n",
    "        ideal_relevances = [1] * min(len(actual), len(recommended))\n",
    "        idcg = np.sum(ideal_relevances / np.log2(np.arange(2, len(ideal_relevances) + 2)))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        logger.debug(f\"NDCG calculation - DCG: {dcg:.4f}, IDCG: {idcg:.4f}, NDCG: {ndcg:.4f}\")\n",
    "        return ndcg\n",
    "\n",
    "    def _average_precision(self, actual: set, recommended: list) -> float:\n",
    "        \"\"\"Calculate Average Precision\"\"\"\n",
    "        relevant = []\n",
    "        for i, event in enumerate(recommended):\n",
    "            if event in actual:\n",
    "                relevant.append(self._precision(actual, recommended[:i+1]))\n",
    "        ap = np.mean(relevant) if relevant else 0\n",
    "        logger.debug(f\"Average Precision calculation - relevant points: {len(relevant)}, AP: {ap:.4f}\")\n",
    "        return ap\n",
    "\n",
    "    def calculate_coverage(self, test_data: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate what percentage of events can be recommended\"\"\"\n",
    "        logger.info(\"Calculating coverage metric\")\n",
    "        all_events = set(self.item_decoder.values())\n",
    "        recommended_events = set()\n",
    "        \n",
    "        for user_id in test_data['user_id'].unique():\n",
    "            recommended_events.update(self._recommend(user_id))\n",
    "        coverage = len(recommended_events) / len(all_events)\n",
    "        logger.info(f\"Coverage: {len(recommended_events)}/{len(all_events)} = {coverage:.4f}\")\n",
    "        return coverage\n",
    "\n",
    "    def calculate_popularity_bias(self, test_data: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate how biased recommendations are toward popular events\"\"\"\n",
    "        logger.info(\"Calculating popularity bias metric\")\n",
    "        event_popularity = test_data['event_id'].value_counts().to_dict()\n",
    "        recommendations_popularity = []\n",
    "        \n",
    "        for user_id in test_data['user_id'].unique():\n",
    "            for event_id in self._recommend(user_id):\n",
    "                recommendations_popularity.append(event_popularity.get(event_id, 0))\n",
    "        \n",
    "        avg_rec_pop = np.mean(recommendations_popularity) if recommendations_popularity else 0\n",
    "        avg_all_pop = np.mean(list(event_popularity.values()))\n",
    "        \n",
    "        bias = avg_rec_pop / avg_all_pop if avg_all_pop > 0 else 0\n",
    "        logger.info(f\"Popularity bias: {bias:.4f} (rec avg: {avg_rec_pop:.2f}, all avg: {avg_all_pop:.2f})\")\n",
    "        \n",
    "        return bias\n",
    "\n",
    "    def _recommend(self, user_id: int) -> List[str]:\n",
    "        \"\"\"Generate recommendations for a single user\"\"\"\n",
    "        logger.debug(f\"Generating recommendations for user {user_id}\")\n",
    "        \n",
    "        try:\n",
    "            user_encoded = self.user_encoder[user_id]\n",
    "            \n",
    "            user_items = csr_matrix((1, len(self.item_encoder)), dtype=np.float32)\n",
    "            \n",
    "            implicit_recs = self.implicit_model.recommend(\n",
    "                userid=user_encoded,\n",
    "                user_items=user_items,\n",
    "                N=self.config.k * 3,\n",
    "                filter_already_liked_items=False\n",
    "            )\n",
    "            \n",
    "            logger.debug(f\"Implicit model returned {len(implicit_recs[0])} recommendations\")\n",
    "            \n",
    "            recommended_events = [self.item_decoder[item] for item in implicit_recs[0]]\n",
    "            organizer_ids = [self._get_organizer_for_event(event) for event in recommended_events]\n",
    "            organizer_encoded = [self.organizer_encoder.get(o, 0) for o in organizer_ids]\n",
    "            \n",
    "            user_array = np.array([user_encoded] * len(implicit_recs[0]))\n",
    "            event_array = np.array(implicit_recs[0])\n",
    "            organizer_array = np.array(organizer_encoded)\n",
    "            \n",
    "            nn_scores = self.nn_model.predict(\n",
    "                [user_array, event_array, organizer_array], \n",
    "                verbose=0\n",
    "            )\n",
    "            nn_scores = self.scaler.inverse_transform(nn_scores.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            combined_scores = implicit_recs[1] * 0.6 + nn_scores * 0.4\n",
    "            top_indices = np.argsort(combined_scores)[::-1][:self.config.k]\n",
    "            \n",
    "            final_recommendations = [self.item_decoder[implicit_recs[0][i]] for i in top_indices]\n",
    "            logger.debug(f\"Generated {len(final_recommendations)} final recommendations\")\n",
    "            \n",
    "            return final_recommendations\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating recommendations for user {user_id}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _get_organizer_for_event(self, event_id: str) -> str:\n",
    "        \"\"\"Helper method to get organizer for an event\"\"\"\n",
    "        organizer = str(hash(event_id) % 1000)\n",
    "        logger.debug(f\"Getting organizer for event {event_id} -> {organizer}\")\n",
    "        return organizer\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Run evaluation and save metrics\"\"\"\n",
    "        logger.info(\"Starting evaluation and saving results\")\n",
    "        \n",
    "        try:\n",
    "            test_data = pd.read_csv(self.config.test_data_path)\n",
    "            metrics = self.evaluate_recommendations(test_data)\n",
    "            \n",
    "            full_results = {\n",
    "                **metrics,\n",
    "                \"model_parameters\": self.config.all_params,\n",
    "                \"num_users\": len(self.user_encoder),\n",
    "                \"num_events\": len(self.item_encoder),\n",
    "                \"num_organizers\": len(self.organizer_encoder),\n",
    "                \"evaluation_time\": pd.Timestamp.now().isoformat()\n",
    "            }\n",
    "            logger.info(f\"Saving results to {self.config.metrics_file_name}\")\n",
    "\n",
    "            \n",
    "            with open(self.config.metrics_file_name, 'w') as f:\n",
    "                json.dump(full_results, f, indent=4)\n",
    "            \n",
    "            logger.info(f\"Evaluation results saved to {self.config.metrics_file_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during evaluation or saving results: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ffcae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 11:39:05,590: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-07-01 11:39:05,592: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-07-01 11:39:05,594: INFO: common: created directory at: artifacts]\n",
      "[2025-07-01 11:39:05,594: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2025-07-01 11:39:05,594: INFO: 2512233877: Initializing HybridRecommenderEvaluator]\n",
      "[2025-07-01 11:39:05,594: INFO: 2512233877: Loading models and encoders]\n",
      "[2025-07-01 11:39:05,627: INFO: 2512233877: Loaded implicit model from artifacts/model_trainer/implicit_model.joblib]\n",
      "[2025-07-01 11:39:05,738: WARNING: saving_utils: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.]\n",
      "[2025-07-01 11:39:05,741: INFO: 2512233877: Loaded neural network model from artifacts/model_trainer/nn_model.h5]\n",
      "[2025-07-01 11:39:05,758: INFO: 2512233877: Loaded scaler from artifacts/model_trainer/scaler.joblib]\n",
      "[2025-07-01 11:39:05,798: INFO: 2512233877: Loaded encoders from artifacts/model_trainer/encoders.joblib]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newst\\miniconda3\\envs\\recommendation-engine-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 11:39:05,798: INFO: 2512233877: Successfully extracted user, item and organizer encoders]\n",
      "[2025-07-01 11:39:05,798: INFO: 2512233877: Created inverse mappings for encoders]\n",
      "[2025-07-01 11:39:05,798: INFO: 2512233877: HybridRecommenderEvaluator initialized successfully]\n",
      "[2025-07-01 11:39:05,798: INFO: 2512233877: Starting evaluation and saving results]\n",
      "[2025-07-01 11:39:05,904: INFO: 2512233877: Starting recommendation evaluation]\n",
      "[2025-07-01 11:39:05,920: INFO: 2512233877: Filtered test data from 50000 to 50000 records]\n",
      "[2025-07-01 11:39:06,024: INFO: 2512233877: Prepared evaluation data for 4999 users]\n",
      "[2025-07-01 11:39:06,024: INFO: 2512233877: Calculating coverage metric]\n",
      "[2025-07-01 11:47:07,759: INFO: 2512233877: Coverage: 4657/10000 = 0.4657]\n",
      "[2025-07-01 11:47:07,760: INFO: 2512233877: Calculating popularity bias metric]\n",
      "[2025-07-01 11:54:59,992: INFO: 2512233877: Popularity bias: 0.9837 (rec avg: 4.95, all avg: 5.03)]\n",
      "[2025-07-01 11:54:59,993: INFO: 2512233877: Initialized metrics dictionary]\n",
      "[2025-07-01 12:03:43,540: INFO: 2512233877: Evaluation completed. Metrics calculated:]\n",
      "[2025-07-01 12:03:43,541: INFO: 2512233877: precision@k: 0.0006]\n",
      "[2025-07-01 12:03:43,542: INFO: 2512233877: recall@k: 0.0006]\n",
      "[2025-07-01 12:03:43,542: INFO: 2512233877: ndcg@k: 0.0005]\n",
      "[2025-07-01 12:03:43,543: INFO: 2512233877: map@k: 0.0009]\n",
      "[2025-07-01 12:03:43,543: INFO: 2512233877: coverage: 0.4657]\n",
      "[2025-07-01 12:03:43,544: INFO: 2512233877: popularity_bias: 0.9837]\n",
      "[2025-07-01 12:03:43,544: INFO: 2512233877: f1_score: 0.0006]\n",
      "[2025-07-01 12:03:43,694: INFO: 2512233877: Saving results to artifacts/model_evaluation/metrics.json]\n",
      "[2025-07-01 12:03:43,705: INFO: 2512233877: Evaluation results saved to artifacts/model_evaluation/metrics.json]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    evaluator = HybridRecommenderEvaluator(config=model_evaluation_config)\n",
    "    evaluator.save_results()\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error during model evaluation\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation-engine-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
