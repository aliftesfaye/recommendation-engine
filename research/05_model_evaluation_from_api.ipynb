{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    implicit_model_path: Path\n",
    "    nn_model_path: Path\n",
    "    scaler_path: Path\n",
    "    encoders_path: Path\n",
    "    all_params: dict\n",
    "    metrics_file_name: Path\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hybrid_recommender.constants import *\n",
    "from src.hybrid_recommender.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        ):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.HybridRecommender\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        return ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            implicit_model_path=config.implicit_model_path,\n",
    "            nn_model_path=config.nn_model_path,\n",
    "            scaler_path=config.scaler_path,\n",
    "            encoders_path=config.encoders_path,\n",
    "            all_params=params,\n",
    "            metrics_file_name=config.metrics_file_name,\n",
    "            k=params.k\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from src.hybrid_recommender import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommenderEvaluator:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.load_models()\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load all required models and encoders\"\"\"\n",
    "        self.implicit_model = joblib.load(self.config.implicit_model_path)\n",
    "        self.nn_model = tf.keras.models.load_model(self.config.nn_model_path)\n",
    "        self.scaler = joblib.load(self.config.scaler_path)\n",
    "        encoders = joblib.load(self.config.encoders_path)\n",
    "        self.user_encoder = encoders['user_encoder']\n",
    "        self.item_encoder = encoders['item_encoder']\n",
    "        self.user_decoder = {i: u for u, i in self.user_encoder.items()}\n",
    "        self.item_decoder = {i: m for m, i in self.item_encoder.items()}\n",
    "\n",
    "    def evaluate_recommendations(self, test_data: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate recommendation quality using standard metrics\"\"\"\n",
    "        # Filter test data to only include known users/items\n",
    "        test_data = test_data[\n",
    "            test_data['User-ID'].isin(self.user_encoder) & \n",
    "            test_data['ISBN'].isin(self.item_encoder)\n",
    "        ]\n",
    "        \n",
    "        # Group by user and get actual interactions\n",
    "        user_items = test_data.groupby('User-ID')['ISBN'].apply(set).to_dict()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for user_id, actual_items in user_items.items():\n",
    "            recommended_items = set(self._recommend(user_id))\n",
    "            relevant_recommended = actual_items & recommended_items\n",
    "            \n",
    "            precision = len(relevant_recommended) / len(recommended_items) if recommended_items else 0\n",
    "            recall = len(relevant_recommended) / len(actual_items) if actual_items else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        avg_precision = np.mean(precisions)\n",
    "        avg_recall = np.mean(recalls)\n",
    "        \n",
    "        return {\n",
    "            \"precision@k\": avg_precision,\n",
    "            \"recall@k\": avg_recall,\n",
    "            \"f1_score\": 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) \n",
    "                        if (avg_precision + avg_recall) > 0 else 0,\n",
    "            \"coverage\": self.calculate_coverage(test_data)\n",
    "        }\n",
    "\n",
    "    def calculate_coverage(self, test_data: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate what percentage of items can be recommended\"\"\"\n",
    "        all_items = set(self.item_decoder.values())\n",
    "        recommended_items = set()\n",
    "        \n",
    "        for user_id in test_data['User-ID'].unique():\n",
    "            recommended_items.update(self._recommend(user_id))\n",
    "        \n",
    "        return len(recommended_items) / len(all_items)\n",
    "\n",
    "    def _recommend(self, user_id: int) -> list:\n",
    "        \"\"\"Generate recommendations for a single user\"\"\"\n",
    "        user_encoded = self.user_encoder[user_id]\n",
    "        \n",
    "        # Get implicit recommendations\n",
    "        implicit_recs = self.implicit_model.recommend(\n",
    "            user_encoded, \n",
    "            None,  # Passing None since we don't need to retrain\n",
    "            N=self.config.k*2\n",
    "        )\n",
    "        \n",
    "        # Score with neural network\n",
    "        user_array = np.array([user_encoded] * len(implicit_recs[0]))\n",
    "        item_array = np.array(implicit_recs[0])\n",
    "        \n",
    "        nn_scores = self.nn_model.predict([user_array, item_array], verbose=0)\n",
    "        nn_scores = self.scaler.inverse_transform(nn_scores.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Combine and sort\n",
    "        combined_scores = implicit_recs[1] * nn_scores\n",
    "        top_indices = np.argsort(combined_scores)[::-1][:self.config.k]\n",
    "        \n",
    "        return [self.item_decoder[item] for item in item_array[top_indices]]\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Run evaluation and save metrics\"\"\"\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "        metrics = self.evaluate_recommendations(test_data)\n",
    "        \n",
    "        # Add model parameters to metrics\n",
    "        full_results = {\n",
    "            **metrics,\n",
    "            \"model_parameters\": self.config.all_params,\n",
    "            \"num_users\": len(self.user_encoder),\n",
    "            \"num_items\": len(self.item_encoder)\n",
    "        }\n",
    "        \n",
    "        save_json(path=self.config.metrics_file_name, data=full_results)\n",
    "        logger.info(f\"Evaluation results saved to {self.config.metrics_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ffcae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    evaluator = HybridRecommenderEvaluator(config=model_evaluation_config)\n",
    "    evaluator.save_results()\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error during model evaluation\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation-engine-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
