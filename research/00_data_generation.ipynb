{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e706934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8df202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\recommendation-engine\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c616ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29003aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\recommendation-engine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a7681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataGenerationConfig:\n",
    "    root_dir: Path\n",
    "    data_dir: Path\n",
    "    target_multiple_bookings: float = 1.5  # 50% more bookings than base count\n",
    "    target_multiple_comments: float = 2.0   # 2x more comments than base count\n",
    "    full_interaction_rate: float = 0.3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551ea2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hybrid_recommender.constants import *\n",
    "from src.hybrid_recommender.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82eca6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self):\n",
    "        config_filepath = CONFIG_FILE_PATH\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_generation_config(self) -> DataGenerationConfig:\n",
    "        config = self.config.data_generation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_generation_config = DataGenerationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_dir=config.data_dir,\n",
    "            \n",
    "        )\n",
    "\n",
    "        return data_generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6a511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.hybrid_recommender import logger\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0ca48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneration:\n",
    "    def __init__(self, config: DataGenerationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def _generate_ids(self, count: int) -> np.ndarray:\n",
    "        \"\"\"Vectorized UUID generation\"\"\"\n",
    "        return np.array([str(uuid.uuid4()) for _ in range(count)])\n",
    "\n",
    "    def _generate_dataset(self, n_rows: int, id_pools: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Generate base dataset with optimized numpy operations\"\"\"\n",
    "        return {\n",
    "            'user_id': np.random.choice(id_pools['user_ids'], n_rows),\n",
    "            'event_id': np.random.choice(id_pools['event_ids'], n_rows),\n",
    "            'organizer_id': np.random.choice(id_pools['organizer_ids'], n_rows)\n",
    "        }\n",
    "\n",
    "    def _create_additional_bookings(self, base_data: Dict[str, np.ndarray], id_pools: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Create additional bookings with optimized vector operations\"\"\"\n",
    "        current_count = len(base_data['user_id'])\n",
    "        additional_count = int(current_count * (self.config.target_multiple_bookings - 1))\n",
    "        \n",
    "        # Select users who will have multiple bookings\n",
    "        multi_book_users = np.random.choice(id_pools['user_ids'], \n",
    "                                          size=int(len(id_pools['user_ids']) * 0.4), \n",
    "                                          replace=False)\n",
    "        \n",
    "        # Get their existing bookings to duplicate\n",
    "        user_bookings = pd.DataFrame(base_data)\n",
    "        eligible = user_bookings[user_bookings['user_id'].isin(multi_book_users)]\n",
    "        \n",
    "        # Sample additional bookings (with replacement to allow multiple duplicates)\n",
    "        additional = eligible.sample(n=additional_count, replace=True)\n",
    "        \n",
    "        # Combine with original\n",
    "        combined = pd.concat([user_bookings, additional], ignore_index=True)\n",
    "        return {col: combined[col].values for col in combined.columns}\n",
    "\n",
    "    def _create_additional_comments(self, base_data: Dict[str, np.ndarray], id_pools: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Create additional comments with optimized vector operations\"\"\"\n",
    "        current_count = len(base_data['user_id'])\n",
    "        additional_count = int(current_count * (self.config.target_multiple_comments - 1))\n",
    "        \n",
    "        # Select active commenters (users who comment more)\n",
    "        active_commenters = np.random.choice(id_pools['user_ids'], \n",
    "                                           size=int(len(id_pools['user_ids']) * 0.6), \n",
    "                                           replace=False)\n",
    "        \n",
    "        # Get their existing comments to duplicate or create new ones\n",
    "        user_comments = pd.DataFrame(base_data)\n",
    "        eligible = user_comments[user_comments['user_id'].isin(active_commenters)]\n",
    "        \n",
    "        # Create additional comments (mix of duplicates and new comments)\n",
    "        dup_count = int(additional_count * 0.7)  # 70% duplicates\n",
    "        new_count = additional_count - dup_count\n",
    "        \n",
    "        # Duplicate existing comments\n",
    "        duplicates = eligible.sample(n=dup_count, replace=True)\n",
    "        \n",
    "        # Create new comments for same users\n",
    "        new_comments = pd.DataFrame({\n",
    "            'user_id': np.random.choice(active_commenters, new_count),\n",
    "            'event_id': np.random.choice(id_pools['event_ids'], new_count),\n",
    "            'organizer_id': np.random.choice(id_pools['organizer_ids'], new_count)\n",
    "        })\n",
    "        \n",
    "        # Combine all comments\n",
    "        combined = pd.concat([user_comments, duplicates, new_comments], ignore_index=True)\n",
    "        return {col: combined[col].values for col in combined.columns}\n",
    "\n",
    "    def _create_full_interactions(self, booking_data: Dict[str, np.ndarray], \n",
    "                                comment_data: Dict[str, np.ndarray], \n",
    "                                like_data: Dict[str, np.ndarray]) -> Tuple[Dict, Dict, Dict]:\n",
    "        \"\"\"Create full interactions (book + comment + like) for selected events\"\"\"\n",
    "        # Convert to DataFrames for easier manipulation\n",
    "        df_book = pd.DataFrame(booking_data)\n",
    "        df_comment = pd.DataFrame(comment_data)\n",
    "        df_like = pd.DataFrame(like_data)\n",
    "        \n",
    "        # Select events for full interactions\n",
    "        event_sample = df_book['event_id'].drop_duplicates().sample(frac=self.config.full_interaction_rate)\n",
    "        full_interactions = df_book[df_book['event_id'].isin(event_sample)]\n",
    "        \n",
    "        # Add matching comments\n",
    "        new_comments = full_interactions[['user_id', 'event_id', 'organizer_id']].copy()\n",
    "        df_comment = pd.concat([df_comment, new_comments], ignore_index=True)\n",
    "        \n",
    "        # Add matching likes\n",
    "        new_likes = full_interactions[['user_id', 'event_id', 'organizer_id']].copy()\n",
    "        df_like = pd.concat([df_like, new_likes], ignore_index=True)\n",
    "        \n",
    "        return (\n",
    "            {col: df_book[col].values for col in df_book.columns},\n",
    "            {col: df_comment[col].values for col in df_comment.columns},\n",
    "            {col: df_like[col].values for col in df_like.columns}\n",
    "        )\n",
    "\n",
    "    def generate_files(self) -> None:\n",
    "        \"\"\"Generate optimized datasets with increased comments and bookings\"\"\"\n",
    "        if not os.path.exists(self.config.data_dir):\n",
    "            os.makedirs(self.config.data_dir, exist_ok=True)\n",
    "            logger.info(f\"Generating enhanced data to: {self.config.data_dir}\")\n",
    "            \n",
    "            np.random.seed(42)\n",
    "            \n",
    "            # Base size parameters\n",
    "            base_size = 100_000\n",
    "            id_pools = {\n",
    "                'user_ids': self._generate_ids(5000),\n",
    "                'event_ids': self._generate_ids(10000),\n",
    "                'organizer_ids': self._generate_ids(1000)\n",
    "            }\n",
    "            \n",
    "            # Generate base datasets\n",
    "            booking_data = self._generate_dataset(base_size, id_pools)\n",
    "            comment_data = self._generate_dataset(base_size, id_pools)\n",
    "            like_data = self._generate_dataset(base_size, id_pools)\n",
    "            \n",
    "            # Enhance with additional bookings and comments\n",
    "            booking_data = self._create_additional_bookings(booking_data, id_pools)\n",
    "            comment_data = self._create_additional_comments(comment_data, id_pools)\n",
    "            \n",
    "            # Create full interactions\n",
    "            booking_data, comment_data, like_data = self._create_full_interactions(\n",
    "                booking_data, comment_data, like_data)\n",
    "            \n",
    "            # Add unique IDs using vectorized approach\n",
    "            datasets = {\n",
    "                'bookings': (booking_data, {'event_id': 'booked_event_id', 'organizer_id': 'booked_event_organizer_id'}),\n",
    "                'comments': (comment_data, {'event_id': 'commented_event_id', 'organizer_id': 'commented_event_organizer_id'}),\n",
    "                'likes': (like_data, {'event_id': 'liked_event_id', 'organizer_id': 'liked_event_organizer_id'})\n",
    "            }\n",
    "            \n",
    "            for name, (data, col_map) in datasets.items():\n",
    "                data[f'{name[:-1]}_id'] = self._generate_ids(len(data['user_id']))\n",
    "                df = pd.DataFrame(data).rename(columns=col_map)\n",
    "                df.to_csv(os.path.join(self.config.data_dir, f'{name}.csv'), index=False)\n",
    "            \n",
    "            logger.info(f\"Generated enhanced datasets with:\")\n",
    "            logger.info(f\"- Bookings: {len(booking_data['user_id'])} (Target: {base_size * self.config.target_multiple_bookings:.0f})\")\n",
    "            logger.info(f\"- Comments: {len(comment_data['user_id'])} (Target: {base_size * self.config.target_multiple_comments:.0f})\")\n",
    "            logger.info(f\"- Likes: {len(like_data['user_id'])}\")\n",
    "            logger.info(f\"- Full interactions: {int(base_size * self.config.full_interaction_rate)} events\")\n",
    "        else:\n",
    "            logger.info(f\"Files already exist in: {self.config.data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c58e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 16:36:20,790: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-06-30 16:36:20,790: INFO: common: created directory at: artifacts]\n",
      "[2025-06-30 16:36:20,790: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2025-06-30 16:36:20,798: INFO: 964962886: Generating enhanced data to: artifacts/data_ingestion/generated_data]\n",
      "[2025-06-30 16:36:25,665: INFO: 964962886: Generated enhanced datasets with:]\n",
      "[2025-06-30 16:36:25,665: INFO: 964962886: - Bookings: 150000 (Target: 150000)]\n",
      "[2025-06-30 16:36:25,665: INFO: 964962886: - Comments: 245397 (Target: 200000)]\n",
      "[2025-06-30 16:36:25,665: INFO: 964962886: - Likes: 145397]\n",
      "[2025-06-30 16:36:25,665: INFO: 964962886: - Full interactions: 30000 events]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_generation_config = config.get_data_generation_config()\n",
    "    data_generation = DataGeneration(config=data_generation_config)\n",
    "    data_generation.generate_files()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation-engine-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
